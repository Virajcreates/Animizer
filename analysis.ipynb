{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82dc3cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import numpy as np\n",
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aab3c813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 1\n",
      "GPU name: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Current CUDA version: 11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Get GPU count\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "\n",
    "# Get GPU name\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Current CUDA version:\", torch.version.cuda)\n",
    "else:\n",
    "    print(\"No GPU detected. Make sure your drivers and CUDA toolkit are properly installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4e1f812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81eb770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "nz = 100  # Size of z latent vector (input to generator)\n",
    "ngf = 32  # Size of feature maps in generator (reduced for 4GB VRAM)\n",
    "ndf = 32  # Size of feature maps in discriminator (reduced for 4GB VRAM)\n",
    "num_epochs = 50  # Number of training epochs (adjust based on time)\n",
    "batch_size = 16  # Small batch size for RTX 3050\n",
    "accumulation_steps = 4  # Effective batch size = 16 * 4 = 64\n",
    "lr = 0.0002  # Learning rate\n",
    "beta1 = 0.5  # Beta1 for Adam optimizer\n",
    "image_size = 64  # Image size (64x64 for memory efficiency)\n",
    "nc = 3  # Number of color channels (RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9f8954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"./data\"\n",
    "celeba_dir = os.path.join(data_root, \"celeba/img_align_celeba/img_align_celeba\")\n",
    "celeba_cropped_dir = os.path.join(data_root, \"celeba/celeba_cropped\")\n",
    "anime_dir = os.path.join(data_root, \"anime/images/images\")\n",
    "output_dir = \"./outputs\"\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(celeba_cropped_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6810dab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess CelebA images (crop faces using MTCNN)\n",
    "def preprocess_celeba_images(input_dir, output_dir, size=(64, 64)):\n",
    "    detector = MTCNN()\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(input_dir, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            faces = detector.detect_faces(img_rgb)\n",
    "            if faces:\n",
    "                x, y, w, h = faces[0]['box']\n",
    "                face = img_rgb[max(0, y):y+h, max(0, x):x+w]\n",
    "                face = cv2.resize(face, size, interpolation=cv2.INTER_AREA)\n",
    "                face = Image.fromarray(face)\n",
    "                face.save(os.path.join(output_dir, filename))\n",
    "            else:\n",
    "                print(f\"No face detected in {filename}\")\n",
    "\n",
    "# Preprocess CelebA (run once, comment out after processing)\n",
    "# preprocess_celeba_images(celeba_dir, celeba_cropped_dir)\n",
    "\n",
    "# Data loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "celeba_dataset = datasets.ImageFolder(os.path.join(data_root, \"celeba/img_align_celeba\"), transform=transform)\n",
    "anime_dataset = datasets.ImageFolder(os.path.join(data_root, \"anime/images\"), transform=transform)\n",
    "celeba_loader = DataLoader(celeba_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "anime_loader = DataLoader(anime_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f61ad45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz + nc, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input, condition):\n",
    "        input = torch.cat([input, condition], dim=1)\n",
    "        return self.main(input)\n",
    "\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input).view(-1)\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Loss, optimizers, and mixed precision scaler\n",
    "criterion = nn.BCELoss()\n",
    "optimizerG = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "scaler = torch.amp.GradScaler(device=\"cuda\")\n",
    "\n",
    "# Labels\n",
    "real_label = 1.0\n",
    "fake_label = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c180a7ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "listdir: path should be string, bytes, os.PathLike or None, not ImageFolder",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43manime_dataset\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[1;31mTypeError\u001b[0m: listdir: path should be string, bytes, os.PathLike or None, not ImageFolder"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(anime_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0fe94bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (celeba_imgs, _) in enumerate(celeba_loader):\n",
    "            anime_iter = iter(anime_loader)\n",
    "            try:\n",
    "                anime_imgs, _ = next(anime_iter)\n",
    "            except StopIteration:\n",
    "                anime_iter = iter(anime_loader)\n",
    "                anime_imgs, _ = next(anime_iter)\n",
    "            \n",
    "            # Ensure shapes are correct\n",
    "            if anime_imgs.size(1) != 3:\n",
    "                raise ValueError(f\"Anime images have incorrect channels: {anime_imgs.shape}\")\n",
    "            if celeba_imgs.size(1) != 3:\n",
    "                raise ValueError(f\"CelebA images have incorrect channels: {celeba_imgs.shape}\")\n",
    "            \n",
    "            celeba_imgs = celeba_imgs.to(device)\n",
    "            anime_imgs = anime_imgs.to(device)\n",
    "            batch_size = celeba_imgs.size(0)\n",
    "            \n",
    "            # Train Discriminator\n",
    "            discriminator.zero_grad(set_to_none=True)\n",
    "            d_loss_total = 0\n",
    "            for _ in range(accumulation_steps):\n",
    "                sub_batch_size = batch_size // accumulation_steps\n",
    "                sub_celeba_imgs = celeba_imgs[:sub_batch_size]\n",
    "                sub_anime_imgs = anime_imgs[:sub_batch_size]\n",
    "                \n",
    "                with autocast():\n",
    "                    real_output = discriminator(sub_anime_imgs)\n",
    "                    real_loss = criterion(real_output, torch.full((sub_batch_size,), real_label, device=device, dtype=torch.float))\n",
    "                    noise = torch.randn(sub_batch_size, nz, 1, 1, device=device)\n",
    "                    fake_imgs = generator(noise, sub_celeba_imgs)\n",
    "                    fake_output = discriminator(fake_imgs.detach())\n",
    "                    fake_loss = criterion(fake_output, torch.full((sub_batch_size,), fake_label, device=device, dtype=torch.float))\n",
    "                    d_loss = (real_loss + fake_loss) / accumulation_steps\n",
    "                scaler.scale(d_loss).backward()\n",
    "                d_loss_total += d_loss.item()\n",
    "            \n",
    "            scaler.step(optimizerD)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Train Generator\n",
    "            generator.zero_grad(set_to_none=True)\n",
    "            g_loss_total = 0\n",
    "            for _ in range(accumulation_steps):\n",
    "                sub_batch_size = batch_size // accumulation_steps\n",
    "                sub_celeba_imgs = celeba_imgs[:sub_batch_size]\n",
    "                \n",
    "                with autocast():\n",
    "                    noise = torch.randn(sub_batch_size, nz, 1, 1, device=device)\n",
    "                    fake_imgs = generator(noise, sub_celeba_imgs)\n",
    "                    fake_output = discriminator(fake_imgs)\n",
    "                    g_loss = criterion(fake_output, torch.full((sub_batch_size,), real_label, device=device, dtype=torch.float)) / accumulation_steps\n",
    "                scaler.scale(g_loss).backward()\n",
    "                g_loss_total += g_loss.item()\n",
    "            \n",
    "            scaler.step(optimizerG)\n",
    "            scaler.update()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}] Batch [{i}/{len(celeba_loader)}] \"\n",
    "                      f\"D Loss: {d_loss_total:.4f} G Loss: {g_loss_total:.4f}\")\n",
    "                \n",
    "                # Save generated images\n",
    "                with torch.no_grad():\n",
    "                    noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "                    fake_imgs = generator(noise, celeba_imgs)\n",
    "                save_image(fake_imgs, f\"{output_dir}/epoch_{epoch+1}_batch_{i}.png\", normalize=True)\n",
    "            \n",
    "            # Clear GPU memory\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save(generator.state_dict(), f\"{checkpoint_dir}/generator_epoch_{epoch+1}.pth\")\n",
    "            torch.save(discriminator.state_dict(), f\"{checkpoint_dir}/discriminator_epoch_{epoch+1}.pth\")\n",
    "            print(f\"Saved checkpoint for epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "031edb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function\n",
    "def generate_anime_image(human_img_path, output_path, model_path=None):\n",
    "    if model_path:\n",
    "        generator.load_state_dict(torch.load(model_path))\n",
    "    generator.eval()\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    # Load and preprocess human image\n",
    "    img = Image.open(human_img_path).convert('RGB')\n",
    "    img = transform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate anime image\n",
    "    noise = torch.randn(1, nz, 1, 1, device=device)\n",
    "    with torch.no_grad():\n",
    "        with autocast():\n",
    "            anime_img = generator(noise, img)\n",
    "    \n",
    "    # Save output\n",
    "    save_image(anime_img, output_path, normalize=True)\n",
    "    \n",
    "    # Display result\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Human Image\")\n",
    "    plt.imshow(Image.open(human_img_path))\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Anime Image\")\n",
    "    plt.imshow(Image.open(output_path))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "415c318e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viraj\\AppData\\Local\\Temp\\ipykernel_40972\\3212543631.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.nn.functional.binary_cross_entropy and torch.nn.BCELoss are unsafe to autocast.\nMany models use a sigmoid layer right before the binary cross entropy layer.\nIn this case, combine the two layers using torch.nn.functional.binary_cross_entropy_with_logits\nor torch.nn.BCEWithLogitsLoss.  binary_cross_entropy_with_logits and BCEWithLogits are\nsafe to autocast.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Main execution\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Uncomment to preprocess CelebA (run once)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# preprocess_celeba_images(celeba_dir, celeba_cropped_dir)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Example inference\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     human_img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/celeba/img_align_celeba/img_align_celeba/000001.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[46], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[0;32m     30\u001b[0m     real_output \u001b[38;5;241m=\u001b[39m discriminator(sub_anime_imgs)\n\u001b[1;32m---> 31\u001b[0m     real_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(sub_batch_size, nz, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     33\u001b[0m     fake_imgs \u001b[38;5;241m=\u001b[39m generator(noise, sub_celeba_imgs)\n",
      "File \u001b[1;32mc:\\Users\\viraj\\Documents\\Virajs Projects\\Animizer\\animeenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\viraj\\Documents\\Virajs Projects\\Animizer\\animeenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\viraj\\Documents\\Virajs Projects\\Animizer\\animeenv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:697\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\viraj\\Documents\\Virajs Projects\\Animizer\\animeenv\\lib\\site-packages\\torch\\nn\\functional.py:3554\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3551\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3552\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[1;32m-> 3554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: torch.nn.functional.binary_cross_entropy and torch.nn.BCELoss are unsafe to autocast.\nMany models use a sigmoid layer right before the binary cross entropy layer.\nIn this case, combine the two layers using torch.nn.functional.binary_cross_entropy_with_logits\nor torch.nn.BCEWithLogitsLoss.  binary_cross_entropy_with_logits and BCEWithLogits are\nsafe to autocast."
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Uncomment to preprocess CelebA (run once)\n",
    "    # preprocess_celeba_images(celeba_dir, celeba_cropped_dir)\n",
    "    \n",
    "    # Train the model\n",
    "    train()\n",
    "    \n",
    "    # Example inference\n",
    "    human_img_path = \"./data/celeba/img_align_celeba/img_align_celeba/000001.jpg\"\n",
    "    output_path = \"./outputs/test_anime.png\"\n",
    "    model_path = f\"{checkpoint_dir}/generator_epoch_{num_epochs}.pth\" if os.path.exists(f\"{checkpoint_dir}/generator_epoch_{num_epochs}.pth\") else None\n",
    "    generate_anime_image(human_img_path, output_path, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9e48a77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anime batch shape: torch.Size([16, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Test data loading\n",
    "for batch in anime_loader:\n",
    "    anime_imgs, _ = batch\n",
    "    print(f\"Anime batch shape: {anime_imgs.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27509b73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "animeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
