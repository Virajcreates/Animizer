{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82dc3cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import numpy as np\n",
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aab3c813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 1\n",
      "GPU name: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Current CUDA version: 11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Get GPU count\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "\n",
    "# Get GPU name\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Current CUDA version:\", torch.version.cuda)\n",
    "else:\n",
    "    print(\"No GPU detected. Make sure your drivers and CUDA toolkit are properly installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4e1f812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81eb770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "nz = 100  # Size of z latent vector (input to generator)\n",
    "ngf = 16  # Size of feature maps in generator (reduced for 4GB VRAM)\n",
    "ndf = 16  # Size of feature maps in discriminator (reduced for 4GB VRAM)\n",
    "num_epochs = 30  # Number of training epochs (adjust based on time)\n",
    "batch_size = 16  # Small batch size for RTX 3050\n",
    "accumulation_steps = 4  # Effective batch size = 16 * 4 = 64\n",
    "lr = 0.0002  # Learning rate\n",
    "beta1 = 0.5  # Beta1 for Adam optimizer\n",
    "image_size = 64  # Image size (64x64 for memory efficiency)\n",
    "nc = 3  # Number of color channels (RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9f8954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"./data\"\n",
    "celeba_dir = os.path.join(data_root, \"celeba/img_align_celeba/img_align_celeba\")\n",
    "celeba_cropped_dir = os.path.join(data_root, \"celeba/celeba_cropped\")\n",
    "anime_dir = os.path.join(data_root, \"anime/images/images\")\n",
    "output_dir = \"./outputs\"\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(celeba_cropped_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6810dab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess CelebA images (crop faces using MTCNN)\n",
    "def preprocess_celeba_images(input_dir, output_dir, size=(64, 64)):\n",
    "    detector = MTCNN()\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(input_dir, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            faces = detector.detect_faces(img_rgb)\n",
    "            if faces:\n",
    "                x, y, w, h = faces[0]['box']\n",
    "                face = img_rgb[max(0, y):y+h, max(0, x):x+w]\n",
    "                face = cv2.resize(face, size, interpolation=cv2.INTER_AREA)\n",
    "                face = Image.fromarray(face)\n",
    "                face.save(os.path.join(output_dir, filename))\n",
    "            else:\n",
    "                print(f\"No face detected in {filename}\")\n",
    "\n",
    "# Preprocess CelebA (run once, comment out after processing)\n",
    "# preprocess_celeba_images(celeba_dir, celeba_cropped_dir)\n",
    "\n",
    "# Data loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "celeba_dataset = datasets.ImageFolder(os.path.join(data_root, \"celeba/img_align_celeba\"), transform=transform)\n",
    "anime_dataset = datasets.ImageFolder(os.path.join(data_root, \"anime/images\"), transform=transform)\n",
    "celeba_loader = DataLoader(celeba_dataset, batch_size=batch_size, shuffle=True, num_workers=4,pin_memory=True,drop_last=True)\n",
    "anime_loader = DataLoader(anime_dataset, batch_size=batch_size, shuffle=True, num_workers=4,pin_memory=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f61ad45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # Downsample condition from [sub_batch_size, 3, 64, 64] to [sub_batch_size, 3, 1, 1]\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv2d(nc, nc, 4, 2, 1, bias=False),  # [sub_batch_size, 3, 64, 64] → [sub_batch_size, 3, 32, 32]\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(nc, nc, 4, 2, 1, bias=False),  # [sub_batch_size, 3, 32, 32] → [sub_batch_size, 3, 16, 16]\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(nc, nc, 4, 2, 1, bias=False),  # [sub_batch_size, 3, 16, 16] → [sub_batch_size, 3, 8, 8]\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(nc, nc, 4, 2, 1, bias=False),  # [sub_batch_size, 3, 8, 8] → [sub_batch_size, 3, 4, 4]\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(nc, nc, 4, 4, 0, bias=False),  # [sub_batch_size, 3, 4, 4] → [sub_batch_size, 3, 1, 1]\n",
    "        )\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz + nc, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input, condition):\n",
    "        # Downsample condition to [sub_batch_size, 3, 1, 1]\n",
    "        condition = self.downsample(condition)  # [sub_batch_size, 3, 64, 64] → [sub_batch_size, 3, 1, 1]\n",
    "        # Concatenate noise and condition\n",
    "        input = torch.cat([input, condition], dim=1)  # [sub_batch_size, nz+nc, 1, 1]\n",
    "        output = self.main(input)\n",
    "        return output\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),        # Input: [4, 3, 64, 64] → [4, 32, 32, 32]\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),   # [4, 32, 32, 32] → [4, 64, 16, 16]\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),  # [4, 64, 16, 16] → [4, 128, 8, 8]\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),  # [4, 128, 8, 8] → [4, 256, 4, 4]\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),        # [4, 256, 4, 4] → [4, 1, 1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input).view(-1)  # [sub_batch_size, 1, 1, 1] → [sub_batch_size]\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Loss, optimizers, and mixed precision scaler\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizerG = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "# Labels\n",
    "real_label = 1.0\n",
    "fake_label = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c180a7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data loaders...\n",
      "Anime batch shape: torch.Size([16, 3, 64, 64])\n",
      "CelebA batch shape: torch.Size([16, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing data loaders...\")\n",
    "for batch in anime_loader:\n",
    "    anime_imgs, _ = batch\n",
    "    print(f\"Anime batch shape: {anime_imgs.shape}\")\n",
    "    break\n",
    "for batch in celeba_loader:\n",
    "    celeba_imgs, _ = batch\n",
    "    print(f\"CelebA batch shape: {celeba_imgs.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df0d32a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anime dataset size: 63565\n",
      "CelebA dataset size: 202599\n"
     ]
    }
   ],
   "source": [
    "print(f\"Anime dataset size: {len(anime_dataset)}\")\n",
    "print(f\"CelebA dataset size: {len(celeba_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a17be529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fe94bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train():\n",
    "    for epoch in range(num_epochs):\n",
    "        start_epoch = time.time()\n",
    "        epoch_iterator = tqdm(enumerate(celeba_loader), total=len(celeba_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for i, (celeba_imgs, _) in epoch_iterator:\n",
    "            start_batch = time.time()\n",
    "            # Data loading time\n",
    "            anime_iter = iter(anime_loader)\n",
    "            try:\n",
    "                anime_imgs, _ = next(anime_iter)\n",
    "            except StopIteration:\n",
    "                anime_iter = iter(anime_loader)\n",
    "                anime_imgs, _ = next(anime_iter)\n",
    "            data_load_time = time.time()\n",
    "            \n",
    "            celeba_imgs = celeba_imgs.to(device)\n",
    "            anime_imgs = anime_imgs.to(device)\n",
    "            batch_size = celeba_imgs.size(0)\n",
    "            \n",
    "            if batch_size < accumulation_steps:\n",
    "                epoch_iterator.set_postfix({\"status\": f\"Skipping batch {i} with size {batch_size}\"})\n",
    "                continue\n",
    "            \n",
    "            # Discriminator training\n",
    "            start_d_train = time.time()\n",
    "            discriminator.zero_grad(set_to_none=True)\n",
    "            d_loss_total = 0\n",
    "            for _ in range(accumulation_steps):\n",
    "                sub_batch_size = batch_size // accumulation_steps\n",
    "                if sub_batch_size == 0:\n",
    "                    break\n",
    "                sub_celeba_imgs = celeba_imgs[:sub_batch_size]\n",
    "                sub_anime_imgs = anime_imgs[:sub_batch_size]\n",
    "                \n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    real_output = discriminator(sub_anime_imgs)\n",
    "                    real_loss = criterion(real_output, torch.full((sub_batch_size,), real_label, device=device, dtype=torch.float))\n",
    "                    noise = torch.randn(sub_batch_size, nz, 1, 1, device=device)\n",
    "                    fake_imgs = generator(noise, sub_celeba_imgs)\n",
    "                    fake_output = discriminator(fake_imgs.detach())\n",
    "                    fake_loss = criterion(fake_output, torch.full((sub_batch_size,), fake_label, device=device, dtype=torch.float))\n",
    "                    d_loss = (real_loss + fake_loss) / accumulation_steps\n",
    "                scaler.scale(d_loss).backward()\n",
    "                d_loss_total += d_loss.item()\n",
    "            \n",
    "            scaler.step(optimizerD)\n",
    "            scaler.update()\n",
    "            end_d_train = time.time()\n",
    "            \n",
    "            # Generator training\n",
    "            start_g_train = time.time()\n",
    "            for _ in range(2):\n",
    "                generator.zero_grad(set_to_none=True)\n",
    "                g_loss_total = 0\n",
    "                for _ in range(accumulation_steps):\n",
    "                    sub_batch_size = batch_size // accumulation_steps\n",
    "                    if sub_batch_size == 0:\n",
    "                        break\n",
    "                    sub_celeba_imgs = celeba_imgs[:sub_batch_size]\n",
    "                    \n",
    "                    with torch.amp.autocast('cuda'):\n",
    "                        noise = torch.randn(sub_batch_size, nz, 1, 1, device=device)\n",
    "                        fake_imgs = generator(noise, sub_celeba_imgs)\n",
    "                        fake_output = discriminator(fake_imgs)\n",
    "                        g_loss = criterion(fake_output, torch.full((sub_batch_size,), real_label, device=device, dtype=torch.float)) / accumulation_steps\n",
    "                    scaler.scale(g_loss).backward()\n",
    "                    g_loss_total += g_loss.item()\n",
    "                \n",
    "                scaler.step(optimizerG)\n",
    "                scaler.update()\n",
    "            end_g_train = time.time()\n",
    "            \n",
    "            end_batch = time.time()\n",
    "            epoch_iterator.set_postfix({\n",
    "                \"D Loss\": f\"{d_loss_total:.4f}\", \n",
    "                \"G Loss\": f\"{g_loss_total:.4f}\", \n",
    "                \"Batch Time\": f\"{end_batch - start_batch:.2f} sec\"\n",
    "            })\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        end_epoch = time.time()\n",
    "        print(f\"Epoch {epoch+1} took {(end_epoch - start_epoch)/60:.2f} minutes\")\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save(generator.state_dict(), f\"{checkpoint_dir}/generator_epoch_{epoch+1}.pth\")\n",
    "            torch.save(discriminator.state_dict(), f\"{checkpoint_dir}/discriminator_epoch_{epoch+1}.pth\")\n",
    "            print(f\"Saved checkpoint for epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "031edb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anime_image(human_img_path, output_path, model_path=None):\n",
    "    if model_path:\n",
    "        generator.load_state_dict(torch.load(model_path))\n",
    "    generator.eval()\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    # Load and preprocess human image\n",
    "    img = Image.open(human_img_path).convert('RGB')\n",
    "    img = transform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate anime image\n",
    "    noise = torch.randn(1, nz, 1, 1, device=device)\n",
    "    with torch.no_grad():\n",
    "        with torch.amp.autocast('cuda'):  # Updated API\n",
    "            anime_img = generator(noise, img)\n",
    "    \n",
    "    # Save output\n",
    "    save_image(anime_img, output_path, normalize=True)\n",
    "    \n",
    "    # Display result\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Human Image\")\n",
    "    plt.imshow(Image.open(human_img_path))\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Anime Image\")\n",
    "    plt.imshow(Image.open(output_path))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "415c318e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:   0%|          | 6/12662 [01:54<67:03:55, 19.08s/it, D Loss=1.1755, G Loss=0.7678, Batch Time=16.01 sec]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Main execution\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Uncomment to preprocess CelebA (run once)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# preprocess_celeba_images(celeba_dir, celeba_cropped_dir)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Example inference\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     human_img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/celeba/img_align_celeba/img_align_celeba/000001.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m start_batch \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Data loading time\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m anime_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43manime_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     anime_imgs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(anime_iter)\n",
      "File \u001b[1;32mc:\\Users\\viraj\\Documents\\Virajs Projects\\Animizer\\animeenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:484\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\viraj\\Documents\\Virajs Projects\\Animizer\\animeenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:415\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\viraj\\Documents\\Virajs Projects\\Animizer\\animeenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1138\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1131\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1138\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Uncomment to preprocess CelebA (run once)\n",
    "    # preprocess_celeba_images(celeba_dir, celeba_cropped_dir)\n",
    "    \n",
    "    # Train the model\n",
    "    train()\n",
    "    \n",
    "    # Example inference\n",
    "    human_img_path = \"./data/celeba/img_align_celeba/img_align_celeba/000001.jpg\"\n",
    "    output_path = \"./outputs/test_anime.png\"\n",
    "    model_path = f\"{checkpoint_dir}/generator_epoch_{num_epochs}.pth\" if os.path.exists(f\"{checkpoint_dir}/generator_epoch_{num_epochs}.pth\") else None\n",
    "    generate_anime_image(human_img_path, output_path, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9e48a77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anime batch shape: torch.Size([16, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Test data loading\n",
    "for batch in anime_loader:\n",
    "    anime_imgs, _ = batch\n",
    "    print(f\"Anime batch shape: {anime_imgs.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27509b73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "animeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
